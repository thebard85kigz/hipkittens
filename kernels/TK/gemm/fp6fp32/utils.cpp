#include "kittens.cuh"
using namespace kittens;


/*
Assembly and intrinsic functions.
*/
using as3_uint32_ptr = uint32_t __attribute__((address_space(3)))*;
using index_t = int;
using int32x4_t = int32_t __attribute__((ext_vector_type(4)));


enum class coherency {
    cache_all = 0,
    cache_global = 1,
    cache_stream = 2,
    non_temporal = 3
};

/*
Load store functions.
*/
extern "C" __device__ void 
llvm_amdgcn_raw_buffer_load_lds(int32x4_t rsrc, // does not change (buffer resource; scalar array?)
                                as3_uint32_ptr lds_ptr, // does not change
                                index_t size, // does not change (16 bytes)
                                index_t voffset, 
                                index_t soffset, 
                                index_t offset,  // does not change (0); instruction offset
                                index_t aux) __asm("llvm.amdgcn.raw.buffer.load.lds"); // cache coherency


// Direct global-to-shared load using buffer load to LDS
template<int axis, bool assume_aligned,
         ducks::rt::all RT, ducks::st::all ST, ducks::gl::all GL,
         ducks::coord::tile COORD = coord<ST>,
         int N_THREADS = WARP_THREADS>
__device__ inline void prefill_swizzled_offsets(
    const GL& src, const COORD& idx, ST& dst, uint32_t* swizzled_offsets)
{

    using T = typename ST::dtype;
    constexpr int memcpy_per_tile =  ST::rows * ST::cols * sizeof(T) / (16 * N_THREADS); // 16 --> 32
    static_assert(memcpy_per_tile > 0, "memcpy_per_tile must be greater than 0. Please decrease the number of threads.");

    using U = typename ST::dtype;
    using U2 = base_types::packing<U>::packed_type;
    const int packed_size = sizeof(U2) / sizeof(U);  // 4 for FP6
    
    constexpr int elem_per_thread = 16 / sizeof(T);  // 8
    constexpr int elem_per_warp = elem_per_thread * kittens::WARP_THREADS; // 512
    const int warp_id = warpid();
    const int row_stride = src.template stride<axis>();

    constexpr int num_warps = N_THREADS / kittens::WARP_THREADS;
    constexpr int num_register_subtiles = kittens::TILE_ROW_DIM<T> * kittens::TILE_COL_DIM<T> / elem_per_warp;
    constexpr int num_register_tiles_per_row = ST::cols / kittens::TILE_COL_DIM<T>;

    #pragma unroll
    for (int i = 0; i < memcpy_per_tile; i++) {

        const int register_tile_id = (warp_id + i * num_warps) / num_register_subtiles;
        const int register_subtile_id = (warp_id + i * num_warps) % num_register_subtiles;

        const int register_subtile_cols = kittens::TILE_COL_DIM<T> / num_register_subtiles;
        const int num_register_subtiles_per_row = num_register_tiles_per_row * num_register_subtiles;
        const int warp_col_offset = ((register_tile_id % num_register_tiles_per_row) * num_register_subtiles + register_subtile_id) * register_subtile_cols;
        const int warp_row_offset = (register_tile_id / num_register_tiles_per_row) * kittens::TILE_ROW_DIM<T>;

        int col_offset = warp_col_offset + (laneid() / 32) * elem_per_thread ;
        int row_offset = warp_row_offset + (laneid() % 32);

        const int offset_in_global = (row_offset * row_stride + col_offset) * sizeof(T);

        swizzled_offsets[i] = offset_in_global;
    }
}


// Direct global-to-shared load using buffer load to LDS
template<int axis, bool assume_aligned,
         ducks::st::all ST, ducks::gl::all GL,
         ducks::coord::tile COORD = coord<ST>,
         int N_THREADS = WARP_THREADS>
__device__ inline void load_global_to_shared_direct_with_swizzled_offsets(
    const GL& src, const COORD& idx, ST& dst, uint32_t* swizzled_offsets)
{

    using T = typename ST::dtype;
    constexpr int bytes_per_memcpy = 16 * N_THREADS;
    constexpr int memcpy_per_tile = ST::rows * ST::cols * sizeof(T) / bytes_per_memcpy;
    static_assert(memcpy_per_tile > 0, "memcpy_per_tile must be greater than 0. Please decrease the number of threads.");
    
    constexpr int elem_per_thread = 16 / sizeof(T);  // e.g., 8 for bf16, 4 for fp32
    constexpr int elem_per_warp = elem_per_thread * kittens::WARP_THREADS;

    const int row_stride = src.template stride<axis>();
    coord<> unit_coord = idx.template unit_coord<axis, 3>();
    T* global_ptr = (T*)&src[unit_coord];
    i32x4 srsrc = make_srsrc(global_ptr, row_stride * ST::rows * sizeof(T));

    const int warp_id = warpid();
    const T* lds_base = &dst.data[0] + (warp_id * elem_per_warp);


    #pragma unroll
    for (int i = 0; i < memcpy_per_tile; i++) {
        const T* lds_elem_ptr = lds_base + (i * N_THREADS * elem_per_thread);
        uintptr_t lds_addr = reinterpret_cast<uintptr_t>(lds_elem_ptr);
        as3_uint32_ptr lds_ptr = (as3_uint32_ptr)(lds_addr);

        llvm_amdgcn_raw_buffer_load_lds(
            srsrc, // buffer resource
            lds_ptr,
            16, // 16 bytes
            swizzled_offsets[i],
            0, 
            0, // instruction offset
            static_cast<index_t>(coherency::cache_all)); // cache coherency
    }
}

/**
 * @brief Load data from a shared tile into a register tile.
 *
 * @tparam RT The register tile type
 * @tparam ST The shared tile type
 * @param dst[out] The destination register tile.
 * @param src[in]  The source shared tile.
 */
 template<ducks::rt::row_layout RT, ducks::st::all ST>
 __device__ inline static void load_lds_reg_row(RT &dst, const ST &src) {
 
     static_assert(RT::height == ST::height, "register tile and shared tile must match height");
     static_assert(RT::width  == ST::width,  "register tile and shared tile must match width");
 
     using T2 = RT::dtype;
     using T  = base_types::packing<T2>::unpacked_type;
     using U  = ST::dtype;
     using U2 = base_types::packing<U >::packed_type;
    //  static_assert(sizeof(U) == 2, "only supporting 16-bit dtypes");
 
     const int laneid = kittens::laneid();
     const int elem_per_thread = 16 / sizeof(U); // 8 
     const uint32_t addr = reinterpret_cast<uintptr_t>(&src.data[laneid * elem_per_thread]);

     const int subtile_stride = kittens::TILE_ROW_DIM<U> * kittens::TILE_COL_DIM<U> * sizeof(U) / 2;
     const int tile_stride = subtile_stride * 2;
     const int row_stride = tile_stride * dst.width;
 
     #pragma unroll
     for(int i = 0; i < dst.height; i++) {

        #pragma unroll
        for(int j = 0; j < dst.width; j++) {
 
            #pragma unroll 
            for (int k = 0; k < 2; k++) {
                asm volatile(
                    "ds_read_b128 %0, %1 offset:%2\n"
                    : "=v"(*reinterpret_cast<float4*>(&dst.tiles[i][j].data[k*4]))
                    : "v"(addr), "i"(i * row_stride + j * tile_stride + k * subtile_stride)
                    : "memory"
                );
             }
         }
     }
 }

// ------------------------------32-packed fp6--------------------------------

// Direct global-to-shared load using buffer load to LDS
template<int axis, bool assume_aligned,
         ducks::rt::all RT, ducks::st::all ST, ducks::gl::all GL,
         ducks::coord::tile COORD = coord<ST>,
         int N_THREADS = WARP_THREADS>
__device__ inline void prefill_swizzled_offsets_fp6(
    const GL& src, const COORD& idx, ST& dst, uint32_t* swizzled_offsets)
{

    using T = typename ST::dtype;
    constexpr int bytes_per_thread = 24;
    constexpr int memcpy_per_tile =  ST::rows * ST::cols / (bytes_per_thread * N_THREADS);
    static_assert(memcpy_per_tile > 0, "memcpy_per_tile must be greater than 0. Please decrease the number of threads.");

    constexpr int bytes_per_warp = bytes_per_thread * kittens::WARP_THREADS; // 16 * 64 = 1024
    const int warp_id = warpid();
    // byte stride
    const int row_stride = src.template stride<axis>() * 6 / 8;

    constexpr int num_warps = N_THREADS / kittens::WARP_THREADS;
    constexpr int num_register_tiles_per_row = ST::cols / kittens::TILE_COL_DIM<T>;

    #pragma unroll
    for (int i = 0; i < memcpy_per_tile; i++) {

        const int register_tile_id = warp_id + i * num_warps;

        const int warp_col_offset = (register_tile_id % num_register_tiles_per_row) * kittens::TILE_COL_DIM<T>;
        const int warp_row_offset = (register_tile_id / num_register_tiles_per_row) * kittens::TILE_ROW_DIM<T>;

        int col_offset = warp_col_offset + (laneid() / 32) * bytes_per_thread;
        int row_offset = warp_row_offset + (laneid() % 32); 
        const int offset_in_global = row_offset * row_stride + col_offset;

        swizzled_offsets[i] = offset_in_global;
    }
}


// Direct global-to-shared load using buffer load to LDS
template<int axis, bool assume_aligned,
         ducks::st::all ST, ducks::gl::all GL,
         ducks::coord::tile COORD = coord<ST>,
         int N_THREADS = WARP_THREADS>
__device__ inline void load_global_to_shared_direct_with_swizzled_offsets_fp6(
    const GL& src, const COORD& idx, ST& dst, uint32_t* swizzled_offsets)
{

    using U = typename ST::dtype;
    constexpr int bytes_per_thread = 24;
    constexpr int memcpy_per_tile =  ST::rows * ST::cols / (bytes_per_thread * N_THREADS);
    static_assert(memcpy_per_tile > 0, "memcpy_per_tile must be greater than 0. Please decrease the number of threads.");
    
    constexpr int elem_per_warp = bytes_per_thread * kittens::WARP_THREADS;

    // byte stride
    const int row_stride = src.template stride<axis>() * 6 / 8;
    coord<> unit_coord = idx.template unit_coord<axis, 3>();
    U* global_ptr = (U*)&src[unit_coord]; // TODO: check if this is correct
    i32x4 srsrc = make_srsrc(global_ptr, row_stride * ST::rows);

    const int warp_id = warpid();
    const U* lds_base = &dst.data[warp_id * elem_per_warp];

    #pragma unroll
    for (int i = 0; i < memcpy_per_tile; i++) {
        const U* lds_elem_ptr = lds_base + (i * N_THREADS * bytes_per_thread);
        uintptr_t lds_addr = reinterpret_cast<uintptr_t>(lds_elem_ptr);
        as3_uint32_ptr lds_ptr = (as3_uint32_ptr)(lds_addr);

        const U* lds_elem_ptr_b32 = lds_elem_ptr + (16 * kittens::WARP_THREADS);
        uintptr_t lds_addr_b32 = reinterpret_cast<uintptr_t>(lds_elem_ptr_b32);
        as3_uint32_ptr lds_ptr_b32 = (as3_uint32_ptr)(lds_addr_b32);

        const U* lds_elem_ptr_b32_next = lds_elem_ptr + (20 * kittens::WARP_THREADS);
        uintptr_t lds_addr_b32_next = reinterpret_cast<uintptr_t>(lds_elem_ptr_b32_next);
        as3_uint32_ptr lds_ptr_b32_next = (as3_uint32_ptr)(lds_addr_b32_next);

        llvm_amdgcn_raw_buffer_load_lds(
            srsrc, // buffer resource
            lds_ptr,
            16, // 16 bytes
            swizzled_offsets[i],
            0, 
            0, // instruction offset
            static_cast<index_t>(coherency::cache_all)); // cache coherency

        llvm_amdgcn_raw_buffer_load_lds(
            srsrc, // buffer resource
            lds_ptr_b32,
            4, // 4 bytes
            swizzled_offsets[i] + 16,
            0, 
            0, // instruction offset
            static_cast<index_t>(coherency::cache_all)); // cache coherency

        llvm_amdgcn_raw_buffer_load_lds(
            srsrc, // buffer resource
            lds_ptr_b32_next,
            4, // 4 bytes
            swizzled_offsets[i] + 20,
            0, 
            0, // instruction offset
            static_cast<index_t>(coherency::cache_all)); // cache coherency
    }
}

/**
 * @brief Load data from a shared tile into a register tile.
 *
 * @tparam RT The register tile type
 * @tparam ST The shared tile type
 * @param dst[out] The destination register tile.
 * @param src[in]  The source shared tile.
 */
 template<ducks::rt::row_layout RT, ducks::st::all ST>
 __device__ inline static void load_lds_reg_row_fp6(RT &dst, const ST &src) {
 
     static_assert(RT::height == ST::height, "register tile and shared tile must match height");
     static_assert(RT::width  == ST::width,  "register tile and shared tile must match width");
 
     using U  = ST::dtype;
     const int laneid = kittens::laneid();
     const uint32_t addr_b128 = reinterpret_cast<uintptr_t>(&src.data[0] + laneid * 16);
     const uint32_t addr_b32 = reinterpret_cast<uintptr_t>(&src.data[0] + kittens::WARP_THREADS * 16 + laneid * 4);
     const uint32_t addr_b32_next = reinterpret_cast<uintptr_t>(&src.data[0] + kittens::WARP_THREADS * 20 + laneid * 4);

     const int tile_stride = kittens::TILE_ROW_DIM<U> * kittens::TILE_COL_DIM<U> * sizeof(U);
     const int row_stride = tile_stride * dst.width;
 
     #pragma unroll
     for(int i = 0; i < dst.height; i++) {

        #pragma unroll
        for(int j = 0; j < dst.width; j++) {
            asm volatile(
                "ds_read_b128 %0, %3 offset:%6\n"
                "ds_read_b32 %1, %4 offset:%6\n"
                "ds_read_b32 %2, %5 offset:%6\n"
                "s_waitcnt lgkmcnt(0)\n"
                : "=v"(*reinterpret_cast<float4*>((&dst.tiles[i][j].data[0]))),
                  "=v"(*reinterpret_cast<float*>((&dst.tiles[i][j].data[0] + 16))),
                  "=v"(*reinterpret_cast<float*>((&dst.tiles[i][j].data[0] + 20)))
                : "v"(addr_b128), "v"(addr_b32), "v"(addr_b32_next), 
                  "i"(i * row_stride + j * tile_stride)
                : "memory"
            );
         }
     }
 }

 template<int axis, ducks::rt::row_layout RT, ducks::gl::all GL, ducks::coord::tile COORD=coord<RT>>
__device__ inline static void store_fp6(const GL &dst, const RT &src, const COORD &idx) {
    using T2 = RT::dtype;
    using U = typename GL::dtype;
    using U2 = base_types::packing<U>::packed_type;

    U *dst_ptr = (U*)&dst[(idx.template unit_coord<axis, 3>())];
    const int row_stride = dst.template stride<axis>() * 6 / 8;
    int laneid = kittens::laneid();

    int row_offset = laneid%32, col_offset = 24*(laneid/32);

    i32x4 srsrc = make_srsrc(dst_ptr, row_stride * RT::rows);

    #pragma unroll
    for(int i = 0; i < src.height; i++) {
        int row = src.tile_size_row*i + row_offset;
        
        #pragma unroll
        for(int j = 0; j < src.width; j++) {
            int col = src.tile_size_col*j + col_offset;
            
            // const __uint128_t tmp = *reinterpret_cast<const __uint128_t*>((&src.tiles[i][j].data[0]));
            const __uint128_t tmp = 1;
            llvm_amdgcn_raw_buffer_store_b128(tmp, srsrc, row*row_stride + col, 0, 0);

            const uint64_t tmp_b64 = 1;
            // const uint64_t tmp_b64 = *reinterpret_cast<const uint64_t*>((&src.tiles[i][j].data[16]));
            llvm_amdgcn_raw_buffer_store_b64(tmp_b64, srsrc, row*row_stride + col + 16, 0, 0);
        }
    }
}

template<ducks::rt::all RT, ducks::gl::all GL, ducks::coord::tile COORD=coord<RT>>
__device__ inline static void store_fp6(const GL &dst, const RT &src, const COORD &idx) {
    store_fp6<2, RT, GL, COORD>(dst, src, idx);
}